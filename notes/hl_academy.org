* Учебник по высоким нагрузкам
Олег Бунин, Константин Осипов, Максим Лапшин, Константин Машуков.

Довольно бестолковая книжка, с хаотичной организацией, в разных главах разной.
Без четких определений терминов (или вообще без определений).
Написано сленгом, со стилистическими ошибками.

Это не учебник, это беседа в баре за пивом.
Но полезная инфа тут есть.


** Урок 1.

*** Монолитные приложения и сервис-ориентированная архитектура.

Монолитные приложения.
плюсы:
- отсутствие накладных расходов на коммуникацию
минусы:
- сложность разработки
- сложность масштабирования разработки (трудно подключить новых разработчиков)
- нет устойчивости к авариям

Сервис-ориентированная архитектура (СОА)
Плюсы и минусы меняются местами.

Примечания:
- Тема раскрыта так себе :)
- Нужно учесть, что добавляются сложности деплоя, конфигурирования и мониторинга.


*** Ремесленный и промышленный подход.

Промышленный подход: средства масштабирования разрабатываются отдельно от бизнес логики.
- долгая разработка общих инструментов
- высокие требования к железу
- быстрая разработка приложений
- для разработки приложений можно использовать программистов низкой квалификации
Facebook, Google, Yandex.

Ремесленный подход: средства масштабирования и бизнес логика разрабатываются одновременно.
- быстрая разработка новых решений
- нужны высококвалифицированные разработчики
- отсутствие накладных расходов на общую шину
- эффективное использование железа
ВКонтакте.


*** Масштабирование архитектурного решения

Вертикальное:
- увеличение мощности железа
- имеет предел
Стоимость машин с более высокими характеристиками не обязательно возрастает линейно.

Горизонтальное:
- подключение дополнительных серверов
- написание софта так, чтобы он мог использовать эти сервера

Диагональное: вертикальное + горизонтальное.

Первое, что необходимо сделать, — это изучить предметную область,
как данные движутся внутри системы, как они обрабатываются, откуда и куда текут.

Масштабирование во времени:
Разные данные имеют разные требования к актуальности.
Отложенная обработка.
Очереди.

Трехзвенная структура:
- Frontend (обработка запросов клиентов)
- Backend (бизнес логика)
- Data Storage


** Урок 2. Масштабирование Frontend.

Обычно фронтенд представляет собой легковесный веб-сервер,
разработчики которого сделали все для того, чтобы каждый запрос
обрабатывался максимально быстро при минимальных затратах
ресурсов. Например, у nginx на 10 тысяч неактивных
keep-alive-соединений уходит не более 2,5 мегабайт памяти.

При разработке самой первой версии nginx шла настоящая борьба за
каждый килобайт, выделяемый на обработку запроса. Благодаря этому
nginx тратит на обработку запроса около 8–10 килобайт.

- отдача статического контента
- кеширование (ответы backend в файлы)
- балансировка нагрузки между backend-узлами
- обслуживание медленных клиентов


*** Кеширование

Отдельно стоит упомянуть о потенциальных проблемах кеширования на
фронтенде. Одна из них — одновременная попытка вычислить просроченное
значение кеша популярной страницы. Если вы кешируете главную страницу,
то при сбрасывании ее значения вы можете получить сразу несколько
запросов к бэкенду на вычисление этой страницы.

В nginx имеется два механизма для решения подобных проблем.

Первый механизм построен на директиве proxy_cache_lock. При ее
использовании только первый запрос вычисляет новое значение элемента
кеша. Все остальные запросы этого элемента ожидают появления ответа в
кеше или истечения тайм-аута.

Второй механизм — мягкое устаревание кеша, когда при определенных
настройках, заданных с помощью директив, пользователю отдается уже
устаревшее значение.


*** Балансировка

Шквал:

Допустим, есть ряд бэкендов, выполняющих однотипные задачи. Запрос
приходит на первый бэкенд, начинает выполняться, но не успевает до
окончания тайм-аута. Умный фронтенд перебрасывает запрос на новый
бэкенд, тот тоже не успевает. Таким образом, очень быстро вся сеть
бэкендов ляжет.

Антишквал:

1. Промежуточное звено с очередью, из которого бэкенды сами забирают задачи.
Проблемы этого варианта:
- Смешение подходов — использование асинхронных методов для решения синхронной задачи.
- Дальнейшее выполнение запроса, когда фронтенд отключился и больше не ждет ответа.
- Исчезновение задач, которые попали на тормозящий бэкенд (это решается рестартом очереди).

2. Умные запросы от фронтенда:
Первый запрос к первому бэкенду идет с тайм-аутом в одну секунду.
Второй запрос идет с тайм-аутом две секунды,
третий — три секунды, а четвертого уже нет, то есть мы ограничиваем количество запросов.

Бэкенд может определять, не перегружен ли он (раз в секунду спрашивать
LA и кешировать его). В начале обработки запроса выполняется проверка.
Если LA слишком высокий, фронтенду отдается Gone Away (штатная ситуа-
ция — переход к другому бэкенду).

Бэкенд получает информацию о том, сколько времени ее ответ будет ждать
фронтенд, сколько времени запрос будет актуален.


*** Медленный клиент

Получает много данных по узкому каналу.
Время тратится не на обработку запроса, а на передачу данных.
И все это время соединение занято, и ресурсы сервера на это соединение заняты.


*** Добавление серверов
(Описано очень хреново.)

До того, как запрос попадает от фронтенда к бэкенду, он сначала должен попасть на фронтенд.
Браузер пользователя должен к какому-то серверу послать запрос.

- DNS-балансировка. Несколько машин зашиты в DNS.
- IPVS
- NAT

Правило, которое применимо при разработке любой крупной системы:
решаем проблемы по мере их появления, каждый раз выбирая наиболее простое решение из всех возможных.
(то есть, действуем реактивно, ибо проактивной стратегии нет).


*** Отказоустойчивость фронтенда

Ставим рядом две машинки, у каждой из которых две сетевых карты.
С помощью одной каждая из них «смотрит в мир»,
с помощью другой они слушают и мониторят друг с друга.
Внешние сетевые карты имеют одинаковые IP-адреса.
Весь поток идет через первую машину.
Как только одна из них умирает, поднимается IP-адрес на второй.
Именно так реализованы CARP (во FreeBSD), Heartbeat (в Linux) и другие соединения подобного рода.


** Урок 3. Масштабирование backend.

Функциональное разбиение, при котором разные части системы,
каждая из которых решает строго свою задачу,
разносятся на отдельные физические серверы.


*** Share Nothing & Stateless

Share Nothing означает, что каждый узел является независимым, самодостаточным
и нет какой-то единой точки отказа.

Stateless означает, что процесс программы не хранит свое состояние.

Пользователь пришел и попал на этот конкретный сервер, и нет никакой
разницы, попал пользователь на этот сервер или на другой. После того
как запрос будет обработан, этот сервер полностью забудет информацию
об этом пользователе. Пользователь вовсе не обязан все свои следующие
запросы отправлять на этот же сервер.

Stateless подразумевает, что данные для каждого запроса нужно брать из хранилища.
Эффективнее хранить часть данных в оперативной памяти.
Но это уход от stateless и от share nothing.


*** Слоистость кода

Программируйте так, чтобы ваш код состоял как бы из слоев и каждый слой отвечал
за какой-то определенный процесс в цепочке обработки данных. Скажем, если у вас идет
работа с базой данных, то она должна осуществляться в одном месте, а не быть разбро-
санной по всем скриптам.

Слоистая схема дает возможность переписывать, выкидывать или добавлять целые слои.

Например, решили вы добавить кеширование или шардирование.
Сделать это просто: надо допилить только одно место — слой хранения данных.


*** Кеширование

Кеш — это скорее способ замазать проблему производительности, а не решить ее.

Критерий эффективности использования -- Hit Ratio,
отношение количества запросов, для которых ответ нашелся в кеше, к общему числу запросов.
Если он низкий (50–60%), значит, у вас есть лишние накладные расходы на поход к кешу.

**** Проблема инвалидации кеша
В общем случае не имеет решения.

Когда обновлять?
- при записи
- при чтении
- в фоне

Обновление на write.
(Я так и не понял, какие тут проблемы. У авторов описывается ситуация, когда значение лежит
одновременно в нескольких кэшах, и его нужно обновить везде. Такая ситуация мне не кажется типичной).

Обновление на read.
Тут есть опасность, что придут одновременно много read за невалидным значением,
и будет много одновременных запросов в БД за одним и тем же значением.

Фоновое обновление.
На write значение в кеше помечается как устаревшее, и в очередь ставится задача на его обновление.
Если придут read до того, как значение обновиться, то эти read получат устаревшее значение.

Это называется деградация функциональности:
вы сознательно идете на то, что некоторые из пользователей получат не самые свежие данные.


**** Проблема старта с непрогретым кешем

- Использовать кеш-хранилище с записью на диск (теряем в скорости);
- Вручную заполнять кэш перед стартом (downtime);
- Пускать пользователей на сайт партиями (downtime для части пользователей)

Такая ситуация наглядно иллюстрирует утверждение о том, что кэш не может решить проблему медленной базы данных.
Отсюда вывод: не занимайтесь кешированием, пока у вас не решены другие проблемы.


** Урок 4. Масштабирование во времени.

*** Отложенные вычисления

Скажем, если клиент сделал модификационный запрос (например, создал
новый пост), то вам предстоит провернуть огромный объем
работы. Недостаточно просто положить пост. Нужно обновить счетчики,
оповестить друзей, разослать электронные уведомления. Хорошая новость:
делать все сразу необязательно.

Сохраните данные в некое промежуточное хранилище, а затем обработайте их с помощью отдельного процесса.

Очень часто асинхронно обсчитывается статистика уникальных посетителей
— раз в день, как правило по ночам, в часы наименьшей загрузки,
запускается скрипт, который берет весь массив данных, накопленных за
день, обрабатывает их и сохраняет уже в другом виде.


*** Очереди

Существует целый класс однотипных задач, для которых важна очередность.

Особый вид хранилища, поддерживающий логику FIFO (первый вошел — первый вышел).
Получаются очереди сообщений и очереди задач.

Один из наиболее популярных сейчас — RabbitMQ.

По большому счету очереди — это пример межсервисной коммуникации, когда один сервис
(публикация поста) ставит задачи другому сервису (рассылка электронной почты).

Идемпотентность — повторное действие не изменит наши данные, если в первый раз все было сделано правильно.

Например, в одном из проектов, который мы разрабатывали, запись
в базу шла через очередь сообщений. Это было очень удобно. Можно было на решардинг и на
другие обслуживающие операции выключить один из кусков базы данных на какое-то время.
Все это время у нас тупо росла очередь сообщений и что-то не записывалось. Потом, когда
базу чинят и снова подключают, очередь рассасывается.

Под очередями сообщений в проектировании веб-проектов могут пониматься две разные вещи.
Во-первых, способ отложить задачу «на потом».
Во-вторых, речь может идти о так называемой общей шине данных.
У нас возникает межсервисная коммуникация, которая помогает разнести вызовы между сервисами.


** Урок 5. Базы данных.

В реальной разработке (и это правило относится ко всем сложным проектам)
предпочтение нужно отдавать тем инструментам, которые знают ваши главные специалисты.

При выборе БД нужно учитывать не только саму БД, но и экосистему вокруг нее, и сообщество.

Представь сразу, что твои таблицы расположены не на одном, а на десяти серверах.
Что они разрезаны, самые старые новости лежат на одном сервере, а новые на другом.
(то есть, нужно изначально исходить из того, что данные будут шардированы).

Запросы к БД должны быть простыми. А сложные манипуляции с данными (join, пересечение и т.д.)
перенести в бизнес логику.


*** Шардинг

Разбиение, нарезка ваших данных по машинам.

Как выбрать принцип разбиения — это отдельный большой вопрос.
Главный принцип — данные должны быть максимально связаны в одном шарде и минимально связаны между шардами.

Например, данные только добавляются, никогда не удаляются.
Наиболее простым способом для этого будет некая парадигма с ящиками.
Один шард — это ящик. Ящик наполнился — открыли следующий, ящик наполнился — открыли следующий.
Таким образом, мы данные каждый раз добавляем в новую и новую машину.
Потом мы уже думаем, что делать со старыми ящиками.

Рассмотрим второй принцип, когда характер нагрузки совсем другой.
Допустим, что данные в разных шардах могут расти по разным законам.
Классический пример с Марком Цукербергом и Lady Gaga на Facebook.
Если вы храните всё о Lady Gaga на компьютере No 69, рано или поздно этот компьютер переполнится.

Если вместе с Lady Gaga на этом же компьютере хранится 10 тысяч
невинных обычных домохозяек, то рано или поздно хранение Lady Gaga на
этом шарде приведет к тому, что домохозяйки получат низкое качество
сервисов, потому что постоянно большой профиль нагрузки будет у Lady
Gaga. Главная особенность такого сюжета — его непредсказуемость,
поэтому нужна достаточно гибкая техника — виртуальные шарды.


*** Виртуальные шарды

Нужно предразбить пространство данных на заведомо огромное, но при этом равномерное
по своей наполненности количество виртуальных шардов. Скажем, 100 тысяч виртуальных
шардов. У тебя есть эта цифра, и изначально ты все эти шарды хранишь на небольшом
количестве машин.

Затем можно добавлять новые машины, и менять схему, как виртуальные шарды соотносятся с реальными машинами.
При этом будет возникать необходимость переносить часть шардов с одной машины на другую. То есть, переливать данные.

Как хранится информация, какой виртуальный шард на какой физической машине находится
и какие данные к какому виртуальному шарду относится?
Один вариант -- центральный диспетчер шардов.
Второй вариант -- конфигурационный файл, одинаковый на всех серверах.
(Стремно как-то, не просто будет изменить конфигурацию).

(А про ZooKeeper и аналоги ничего не сказано).


*** Репликация

(Описано вообще невнятно, пришлось описывать самому)

master-master
любой запрос (read/write) можно отправлять на любую машину
Годится, если write редки, а read частые.

master-slave
write можно делать только на master, read на любую машину

Можно построить древовидную структуру, в которой
промежуточные узлы являются master для дочерних узлов, и slave для родительских узлов.


*** Партиционирование

Данные разных типов храняться в разных местах и по-разному.
- В разных таблицах
- В разных БД
- В разных типах БД


*** Кластеризация

Вы в каком-то смысле аутсорсите проблему масштабирования базы данных
на разработчика этой самой базы данных или на разработчика кластера.
Для вас кластер выглядит как единое целое.

Ты покупаешь кластер, его настраивают и далее это решение самостоятельно.
Все внутренние процессы могут быть тебе даже не известны.
Это хорошо, с одной стороны — за тебя все настроили профессионалы.
С другой стороны это плохо — у тебя нет возможности что-либо исправить в случае ошибки.
Ты просто не знаешь, как эта штука работает.


*** Денормализация

Намеренное приведение структуры БД в состояние, не соответствующее критериям нормализации.
Обычно проводится с целью ускорения чтения из БД, за счет добавления избыточных данных.

Есть огромное количество решений, которые хранят данные в денормализованном виде,
но могут представлять их в реляционном виде.
(жаль, нет ни одного примера :)


*** Особенности хранимых процедур в MySQL

Читается из таблицы, компилируется и кешируется к скомпилированном виде в каждом соединении к серверу.

(Непонятно, зачем так делать?
Если компилируются все процедуры, что есть, то лучше сделать это централизовано.
Вероятно, они компилируются по требованию, и хранятся только те, что реально используются.
Тогда есть смысл делать это для каждого соединения отдельно).

Это увеличивает объем памяти, который требуется каждому соединению.

Хранимые процедуры -- это дополнительный уровень абстракции между данными и клиентом.
Он скрывает данные от клиента, и позволяет менять схему, не меняя интерфейса к ней.
(Типа как публичный интерфейс класса в ООП).

Важно учитывать то, что при изменении хранимой процедуры одновременно инвалидируются
все кеши всех активных соединений. Это может привести к серьёзным «провалам» в произ-
водительности, т.к. все соединения одновременно будут пытаться пересоздать свои скомпи-
лированные копии хранимых процедур.
(Замечательная архитектура у MySQL :)


** Урок 6. Надежность, эксплуатация, паттерны масштабируемых архитектур.

Серверный диск, при его активном использовании, в среднем выходит из
строя раз в два года. Это означает, что из трех тысяч серверов прямо
сегодня полетят диски у 4-х машин. Вывод – в большой крупной системе
всегда что-то не работает, какой-то из серверов вышел из строя,
какой-то из дисков сбоит. И это – нормальное состояние системы.


*** Надежность

Заключается в способности сохранять в пределах установленной нормы значения всех параметров,
характеризующих способность выполнять требуемые функции.

Один из способов борьбы с резко возросшей нагрузкой -- деградация функциональности.

В любом проекте есть целый ряд функций, почти незаметных пользователю, но требующих
для своей реализации серьезных серверных ресурсов.

Например, скорость появления опубликованной редактором новости на сайте. Если это
не новостное издание, то пострадает ли пользователь, если новость станет доступна только
через десять минут после публикации? Нет, не пострадает, он даже этого не заметит.
Но разработчикам это позволит внедрить кеширование или прегенерацию.


*** Избыточность и дублирование

Чтобы понять, что именно нужно дублировать, нужно смоделировать проблемное состояние
и заранее просчитать его.

Должно ли программное обеспечение самостоятельно принимать решение
о переключении на другой сервер базы данных?
Это непростой вопрос.

Софт не может отличить падение узла от медленного узла.
Переключиться на другую БД должны все узлы бэкенда.
Иначе будет net split.

Поэтому автоматическое переключение на реплику используется редко. Обычно на реплику
переключают только чтение из базы данных, одновременно отключая (деградация функци-
ональности, это тоже она) запись в базу данных.


*** Принципы надежности

Для фронтенда это балансировка или составление пар серверов, в которых один всегда запасной.

Для бекенда это гомогенные взаимозаменяемые бекенды, share nothing, stateless.

Для баз данных это денормализация, репликация, кластер.

Также надо избегать точек отказа и в железе.
Старайтесь не использовать никакого уникального оборудования без крайней на то нужды.
Стандартное железо:
- Взаимозаменяемость компонент
- Закупка новых серверов и комплектующих не является проблемой

Chaos Monkey тестирование.
Работа обезьянки Хаоса состоит в том, чтобы хаотично прибивать
случайный сервис или процесс на каком-то случайно выбранном сервере.
Весь проект при этом должен продолжать работать.


*** Мониторинг

Второй ключевой аспект крупного проекта – вы должны знать о нем абсолютно все!

Все, что происходит в любой точке системы, любые аномалии в поведении отдельных элемен-
тов вашей программной системы должны оперативно детектироваться и анализироваться.

Джентельменский набор:
- load averages
- количество чтений/записи с диска
- свободное пространство на дисках
- количество процессов
- трафик

Бизнес-параметры:
- количество регистраций за последнюю минуту
- количество отправляемых сообщений
- количество поисковых запросов
- и другие

Специализированные технические параметры:
- время отдачи отдельных страниц
- задержка появления новых данных на реплике
- время выполнения отдельных операций в базе данных

Мониторинг бизнес-показателей -- это основной инструмент прогнозирования нагрузки.
Это поможет вам реагировать проактивно, добавлять ресурсы заранее и в нужных местах.

Олег Илларионов из ВКонтакте рассказал о том, что они постоянно
мониторят отдачу картинок и иногда вручную обрабатывают рост интереса
к какой-то конкретной картинке.  Примером такой картинки Олег назвал
аватарку Павла Дурова, что вовсе не удивительно!  Кстати, аватарка
Павла Дурова не лежит в стандартной системе хранения и доставки
контента ВКонтакте. Это статика, разложенная на все фронтенды, и
отдается вместе со всеми CSS и JS-файлами и другими картинками
оформления.


*** Восстановление

Все средства восстановления должны быть автоматизированы.
Если ты сталкиваешься с какой-то проблемой более, чем один раз – автоматизируйте ее.
Но большую часть автоматизируйте априори.

Масштаб:

Один сервер.
Администрируется одним специалистом, который владеет полной информации о состоянии этого сервера.
Все умещается у него в голове.

5 серверов.
Подходы те же самые, но уже начинается обобщение и автоматизация.
Появляются скрипты, решающие ту или иную задачу системного администрирования.

Больше 20-ти машин.
Возникают проблемы из-за разницы конфигураций и разных версий компонентов.
Требуется большое количество документации. Любое изменение должно быть описано.
Стоимость поддержки сильно превышает стоимость внесения изменений.
Вы перестаете вносить изменения, вы постоянно исправляете ошибки.

Приходит понимание, что надо управлять не одной машиной, а кластером.
Потребуется ввести автоматическую установку машин,
автоматическое управление конфигурациями,
автоматическое развертывание сервисов,
и автоматическую выкатку.


*** Deployment

Нужно уметь перевыкатывать весь свой сайт на новое голое железо за 20 минут
(без учета времени копирования данных).

Amazon очень правильно учит людей. Когда вы берете машину EC2, она может
перезагрузиться в любую секунду, и там не останется ничего, никаких данных,
и нужно иметь возможность выкатываться на голый Linux в течение минут.

Существует большое количество инструментов для деплоймента,
изучите и выберете оптимальный для конкретной ситуации.


*** Процесс отката

Что делать, если обновление, которое вы только что выкатили, сломалось в боевых условиях?
Нужно оперативно и быстро откатиться назад.

Автоматизация процесса отката – это чистая магия.
Основная проблема отката это изменения в базе данных.

Отсюда вывод – любое изменение в базе данных
должно быть оформлено в виде файла с конкретными SQL-командами.
Этот файл должен быть положен в репозиторий
и именно этот файл выполняет скрипт деплоймента
для выкатки новой версии схемы базы данных.

Там же (или в отдельном файле) могут быть SQL-команды для отката к предыдущему состоянию схемы СУБД.

Программный код не должен ломаться от того, что пришел лишний столбец
или, наоборот, нужно столбца в базе данных не существует.
Операция деплоя не атомарна, ситуация, когда код уже новый,
а изменения в СУБД до реплики не докатились, теоретически возможна. (или наоборот)
