* Mikito Takada​ «Distributed systems. for fun and profit».
http://book.mixu.net/distsys/single-page.html


** Introduction

Introducing the key concepts.
Providing a narrative that covers things in enough detail that you get a gist of what's going on without getting stuck on details.


** Distributed systems at a high level

Distributed programming is the art of solving the same problem
that you can solve on a single computer using multiple computers.
Usually, because the problem no longer fits on a single computer.

There are two basic tasks that any computer system needs to accomplish:
- storage and
- computation

Upgrading hardware is a viable strategy.
However, as problem sizes increase
you will reach a point where
either the hardware upgrade that allows you to solve the problem on a single node does not exist,
or becomes cost-prohibitive.
At that point, I welcome you to the world of distributed systems.

It is a current reality that the best value is in mid-range, commodity hardware.
The performance gap between high-end and commodity hardware decreases with cluster size.

It's worthwhile to study distributed algorithms -
they provide efficient solutions to specific problems,
as well as guidance about what is possible,
what the minimum cost of a correct implementation is,
and what is impossible.


*** Scalability
is the ability of a system, network, or process,
to handle a growing amount of work in a capable manner
or its ability to be enlarged to accommodate that growth.
https://en.wikipedia.org/wiki/Scalability


*Size scalability*
Adding more nodes should make the system linearly faster;
growing the dataset should not increase latency.

*Geographic scalability*
It should be possible to use multiple data centers
to reduce the time it takes to respond to user queries,
while dealing with cross-data center latency in some sensible manner.

*Administrative scalability*
Adding more nodes should not increase the administrative costs of the system
(e.g. the administrators-to-machines ratio).


*** Performance
is characterized by the amount of useful work
accomplished by a computer system
compared to the time and resources used.
https://en.wikipedia.org/wiki/Computer_performance

One or more of the following:
- Short response time/low latency for a given piece of work
- High throughput (rate of processing work)
- Low utilization of computing resource(s)

There are tradeoffs involved in optimizing for any of these outcomes.

For example, a system may achieve a higher throughput
by processing larger batches of work.
The tradeoff would be longer response times
for individual pieces of work due to batching.


*** Latency
delay, a period between the initiation of something and the occurrence.

Latency is really the time between when something happened
and the time it has an impact or becomes visible.

What matters for latency is not the amount of old data,
but rather the speed at which new data "takes effect" in the system.
For example, latency could be measured in terms of
how long it takes for a write to become visible to readers.

The other key point based on this definition is that if nothing happens, there is no "latent period".
A system in which data doesn't change doesn't (or shouldn't) have a latency problem.

In a distributed system, there is a minimum latency that cannot be overcome:
the speed of light limits how fast information can travel,
and hardware components have a minimum latency cost incurred per operation.


*** Availability
the proportion of time a system is in a functioning condition.
If a user cannot access the system, it is said to be unavailable.

Distributed systems can take a bunch of unreliable components,
and build a reliable system on top of them.

Systems that have no redundancy can only be as available as their underlying components.
Systems built with redundancy can be tolerant of partial failures and thus be more available.

Availability = uptime / (uptime + downtime).

Availability from a technical perspective is mostly about being fault tolerant.
Because the probability of a failure occurring increases with the number of components,
the system should be able to compensate
so as to not become less reliable as the number of components increases.

How much downtime is allowed per year?
90% ("one nine") 	More than a month
99% ("two nines") 	Less than 4 days
99.9% ("three nines") 	Less than 9 hours
99.99% ("four nines") 	Less than an hour
99.999% ("five nines") 	~ 5 minutes
99.9999% ("six nines") 	~ 31 seconds


*** Fault tolerance
ability of a system to behave in a well-defined manner once faults occur.

Define what faults you expect
and then design a system that is tolerant of them.
You can't tolerate faults you haven't considered.

That's the difference between an error and an anomaly -
an error is incorrect behavior,
while an anomaly is unexpected behavior.


*** What prevents us from achieving good things?

Distributed systems are constrained by two physical factors:
- the number of nodes (which increases with the required storage and computation capacity)
- the distance between nodes (information travels, at best, at the speed of light)


An increase in the number of independent nodes
- increases the probability of failure in a system
  (reducing availability and increasing administrative costs)
- may increase the need for communication between nodes
  (reducing performance as scale increases)

An increase in geographic distance
increases the minimum latency for communication between distant nodes
(reducing performance for certain operations)

Beyond these tendencies -
which are a result of the physical constraints -
is the world of system design options.

Both performance and availability are defined by the external guarantees the system makes.

SLA (service level agreement) for the system:
- if I write data, how quickly can I access it elsewhere?
- After the data is written, what guarantees do I have of durability?
- If I ask the system to run a computation, how quickly will it return results?
- When components fail, or are taken out of operation, what impact will this have on the system?


*** Abstractions and models

Abstractions make things more manageable
by removing real-world aspects
that are not relevant to solving a problem.

Models describe the key properties of a distributed system in a precise manner.

System model (asynchronous / synchronous)
Failure model (crash-fail, partitions, Byzantine)
Consistency model (strong, eventual)

A system that makes weaker guarantees has more freedom of action,
and hence potentially greater performance -
but it is also potentially hard to reason about.
People are better at reasoning about systems that work like a single system, rather than a collection of nodes.


*** Design techniques: partition and replicate

There are two basic techniques that can be applied to a data set.

It can be split over multiple nodes (partitioning)
to allow for more parallel processing.

It can also be copied or cached on different nodes
to reduce the distance between the client and the server
and for greater fault tolerance (replication).

Divide and conquer - I mean, partition and replicate.
./img/part-repl.png

*Partitioning* is dividing the dataset into smaller distinct independent sets;
this is used to reduce the impact of dataset growth since each partition is a subset of the data.

Partitioning improves performance by limiting the amount of data to be examined
and by locating related data in the same partition.

Partitioning improves availability by allowing partitions to fail independently,
increasing the number of nodes that need to fail before availability is sacrificed.

Partitioning is mostly about defining your partitions
based on what you think the primary access pattern will be,
and dealing with the limitations that come from having independent partitions
(e.g. inefficient access across partitions, different rate of growth etc.).

*Replication* is making copies of the same data on multiple machines.

Replication improves performance by making additional computing power and bandwidth
applicable to a new copy of the data.

Replication improves availability by creating additional copies of the data,
increasing the number of nodes that need to fail before availability is sacrificed.

Replication is also the source of many of the problems,
since there are now independent copies of the data
that has to be kept in sync on multiple machines -
this means ensuring that the replication follows a consistency model.

Only one consistency model for replication - strong consistency -
allows you to program as-if the underlying data was not replicated.
Other consistency models expose some internals of the replication to the programmer.

However, weaker consistency models can provide lower latency and higher availability -
and are not necessarily harder to understand, just different.


** Up and down the level of abstraction

There is a tension between the reality that there are many nodes
and with our desire for systems that "work like a single system".
That means finding a good abstraction
that balances what is possible
with what is understandable and performant.

What do we mean when say X is more abstract than Y?
First, that X does not introduce anything new or fundamentally different from Y.
In fact, X may remove some aspects of Y or present them in a way that makes them more manageable.
Second, that X is in some sense easier to grasp than Y,
assuming that the things that X removed from Y are not important to the matter at hand.

Abstractions, fundamentally, are fake.
Every situation is unique, as is every node.
But abstractions make the world manageable.

Indeed, if the things that we kept around are essential,
then the results we can derive will be widely applicable.

All abstractions ignore something in favor of equating things that are in reality unique.
The trick is to get rid of everything that is not essential.
How do you know what is essential?
Well, you probably won't know a priori.

Every time we exclude some aspect of a system
from our specification of the system,
we risk introducing a source of error and/or a performance issue.

A system model is a specification of the characteristics we consider important;
having specified one, we can then take a look at some impossibility results and challenges.


*** System Model
