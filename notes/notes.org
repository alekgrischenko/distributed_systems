* Scaling Erlang
http://inaka.net/blog/2011/10/07/scale-test-plan-simple-erlang-application/

Creating a scalable system is not a matter of just writing it in Erlang.


* Distributed erlang
http://emauton.org/disterl/

The protocol Erlang nodes use to communicate is described between the
[[http://www.erlang.org/doc/apps/erts/erl_dist_protocol.html][distribution protocol]] and [[http://www.erlang.org/doc/apps/erts/erl_ext_dist.html][external term format]] sections of the [[http://www.erlang.org/doc/apps/erts/users_guide.html][ERTS user guide]].



* Обсуждение riak_core

https://groups.google.com/forum/#!msg/erlang-russian/TVbVHhdS9lM/FLHGDrjAzbYJ

Valery Meleshkin:
   Плюсы:
   1) Куча всего уже сделана за вас и довольно качественно
   2) Каждый релиз riak привносит новый функционал, старый ломают крайне редко. Даже приватный API почти не ломают.
   3) Разработчики нормально общаются в твиторе и на GH (фиксят к сожалению не всегда оперативно)
   Минусы:
   1) Башо тестируют каждый релиз riak на определённых версиях OTP. Использовать другие версии будет затруднительно потому что их часто прибивают гвоздями в rebar.config.
   2) Многие депенды прибиты гвоздями к форкам basho, которые как правило отстают от апстрима. Но они постепенно отрезают такие депенды.
   Рекомендации
   1) Строго рекомендуется понимать что вы делаете и как это себя на самом деле ведёт (ownership и hinted handoff, forwarding во всех его ипостасях, разделения сети, ...)
   2) riak_core это AP система, хорошо подумайте подходит ли вам это. Если вам всё таки требуется CP — нужно кроме riak_core взять ещё и riak_ensemble.
   3) Пишите quickcheck/proper модели — это практически единственный способ что-то достоверно протестировать.
   4) Подпишитесь на репозитории basho на GH.
   5) Нормальный map-reduce называется riak_pipe.


* Про наше использование Riak
http://lionet.livejournal.com/98362.html

У нас 50 машин с десятком гигабайт памяти на каждой.
Используется InnoDB сторадж. Каждая машина почти упирается в диск на запись.

У нас этот кластер обслуживает несколько сотен гигабайт данных (сотни миллионов ключей),
в него происходит запись и чтение от нескольких сот до нескольких тысяч раз в секунду.

В течение месяца одна или две риаковских ноды обязательно выйдут из строя ...
Мы этих вылетов не замечаем, продакшн продолжает работать.
Так что операционно риак — это достаточно удобная сущность.
Ночью никого не будит, если хардвер слёг.

Проблемы же с риаком такие:

1. Довольно медленный map/reduce; почти не юзабельный на практике на больших данных.
   Кому нужен MR — используйте хадуп что-ли.

2. Links и link-walking — это шутка такая. Использовать её нельзя, если _максимальное_ количество линков больше чем примерно десяток.
   Графовую базу из риака не построишь.

3. Расширять кластер удобно от трёх до четырёх нод. Добавил ноду и всё.
   От десяти до пятидесяти нод расширять — это большой геморрой, ибо ребалансировка происходит долго
   (добавь ноду — балансируй, добавь другую — балансируй: это может длится неделями).
   В итоге нам пришлось заказывать платную помощь, которая нам расширила кластер, по сути, руками (полуавтоматически).

Весь датасет должен быть в памяти (у вас датасет на диске и всё ок? Так вам риак не особо нужен).


* Страх и ненависть в распределённых системах
https://habrahabr.ru/post/322876/

Распределённая система — это такая шутка, которая состоит из нескольких частей,
и эти части общаются друг с другом.
Но всё усложняется тем, что они общаются с задержками и с ошибками.

Пример из практики с записью одного значения в регистр.

CAP -- это теорема, которая формально не теорема, а эмпирическое правило, но все её называют теоремой.

Consistency. Существует порядка 50 видов целостности.
В CAP имеется в виду самый строгий из них -- линеаризуемость.
«Существует непротиворечивая история последовательных операций».

В реальном мире есть два выхода.
Вы можете либо сместиться в целостности и потерять доступность,
либо сместиться к доступности, но потерять целостность.

Есть много алгоритмов, которые реализуют различные системы CP / AP / AC.
Двухфазный коммит, Paxos, кворум и прочие рафты, Gossip и прочие кучи алгоритмов.

Пример master-slave репликации на Scala.

Jepsen – ое-фреймворк для тестирования распределённых систем. Написан на Clojure.
https://jepsen.io/
Набор готовых тестов для уже существующих баз данных, очередей и прочего. Вы всегда можете написать свой.
Куча статей о найденных проблемах, наверное, во всех базах данных https://aphyr.com/tags/jepsen

имитирует сетевые ошибки
генерирует случайные операции к вашей распределённой системе
сравнивает результат с эталонной моделью

Наша задача с тестом Jepsen — это писать в master, читать с
MasterSlave и понять, как обстоят дела с целостностью и правильно ли
мы написали нашу мастер/слейв репликацию или, может быть, нет.

Jepsen написан на Clojure, и тесты нужно писать тоже Clojure.
Если бы была возможность писать их на чём-нибудь другом, было бы классно.

3й пример -- консенсус. К-во нод которые подтвердили write, или отдали read.
На 5-ти секундном тесте все было хорошо. На 15-ти секундном вылезли проблемы.

PAXOS довольно сложный. Она написана математиком для математиков. Если
вы попытаетесь его реализовать, у вас будет очередная вариация
реализации PAXOS, которых очень много. Она не совсем для людей, она не
разбита на фазы. Это просто огромная портянка, описывающая
математические конструкции. Надо многое додумывать.

RAFT более новый, там учтены все проблемы, там описаны все те шаги,
которые нужно сделать, чтобы реализовать хороший алгоритм
консенсуса. Там всё классно. Есть огромное количество разных
реализаций.

Алгоритмы консенсуса используются много где, даже в третьей версии
MongoDB появился RAFT. В Cassandra всю жизнь был PAXOS. Во многих
базах данных, системах очередей, когда они дорастают до зрелости, рано
или поздно появляется алгоритм консенсуса.

Когда вы пишете свою распределённую систему есть много путей. Вы
должны знать, что эти пути есть и не делать еще один свой.
Каждый путь — это компромисс между целостностью, задержками, пропускной
способностью и прочими разными штуками.
